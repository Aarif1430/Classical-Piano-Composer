{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidfoster/.virtualenvs/gdl/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import numpy\n",
    "from music21 import note, chord\n",
    "\n",
    "from keras.layers import LSTM, Input, Dropout, Dense, Activation, Embedding, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from utils import get_distinct, create_lookups, prepare_sequences, get_music_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 files in total\n",
      "1 Parsing midi_songs/cs1-2all.mid\n",
      "2 Parsing midi_songs/cs5-1pre.mid\n",
      "3 Parsing midi_songs/cs4-1pre.mid\n",
      "4 Parsing midi_songs/cs3-5bou.mid\n",
      "5 Parsing midi_songs/cs1-4sar.mid\n",
      "6 Parsing midi_songs/cs2-5men.mid\n",
      "7 Parsing midi_songs/cs3-3cou.mid\n",
      "8 Parsing midi_songs/cs2-3cou.mid\n",
      "9 Parsing midi_songs/cs1-6gig.mid\n",
      "10 Parsing midi_songs/cs6-4sar.mid\n",
      "11 Parsing midi_songs/cs4-5bou.mid\n",
      "12 Parsing midi_songs/cs4-3cou.mid\n",
      "13 Parsing midi_songs/cs5-3cou.mid\n",
      "14 Parsing midi_songs/cs6-5gav.mid\n",
      "15 Parsing midi_songs/cs6-6gig.mid\n",
      "16 Parsing midi_songs/cs2-1pre.mid\n",
      "17 Parsing midi_songs/cs3-1pre.mid\n",
      "18 Parsing midi_songs/cs3-6gig.mid\n",
      "19 Parsing midi_songs/cs2-6gig.mid\n",
      "20 Parsing midi_songs/cs2-4sar.mid\n",
      "21 Parsing midi_songs/cs3-4sar.mid\n",
      "22 Parsing midi_songs/cs1-5men.mid\n",
      "23 Parsing midi_songs/cs1-3cou.mid\n",
      "24 Parsing midi_songs/cs6-1pre.mid\n",
      "25 Parsing midi_songs/cs2-2all.mid\n",
      "26 Parsing midi_songs/cs3-2all.mid\n",
      "27 Parsing midi_songs/cs1-1pre.mid\n",
      "28 Parsing midi_songs/cs5-2all.mid\n",
      "29 Parsing midi_songs/cs4-2all.mid\n",
      "30 Parsing midi_songs/cs5-5gav.mid\n",
      "31 Parsing midi_songs/cs4-6gig.mid\n",
      "32 Parsing midi_songs/cs5-6gig.mid\n",
      "33 Parsing midi_songs/cs5-4sar.mid\n",
      "34 Parsing midi_songs/cs4-4sar.mid\n",
      "35 Parsing midi_songs/cs6-3cou.mid\n"
     ]
    }
   ],
   "source": [
    "music_type = 'local'\n",
    "mode = 'build'\n",
    "\n",
    "if mode == 'build':\n",
    "    music_list, parser = get_music_list(music_type)\n",
    "    print(len(music_list), 'files in total')\n",
    "\n",
    "    notes = []\n",
    "    durations = []\n",
    "\n",
    "    for i, file in enumerate(music_list):\n",
    "        print(i+1, \"Parsing %s\" % file)\n",
    "        original_score = parser.parse(file).chordify()\n",
    "        intervals = range(-6,5,1)\n",
    "\n",
    "        for interval in intervals:\n",
    "\n",
    "            score = original_score.transpose(interval)\n",
    "\n",
    "            notes.append('START')\n",
    "            durations.append(0)\n",
    "\n",
    "            for element in score.flat:\n",
    "                \n",
    "                if isinstance(element, note.Note):\n",
    "                    if element.isRest:\n",
    "                        notes.append(str(element.name))\n",
    "                        durations.append(element.duration.quarterLength)\n",
    "                    else:\n",
    "                        notes.append(str(element.nameWithOctave))\n",
    "                        durations.append(element.duration.quarterLength)\n",
    "\n",
    "                if isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(n.nameWithOctave for n in element.pitches))\n",
    "                    durations.append(element.duration.quarterLength)\n",
    "\n",
    "            notes.append('END')\n",
    "            durations.append(0)\n",
    "\n",
    "    with open('data/notes', 'wb') as filepath:\n",
    "            pickle.dump(notes, filepath) #['G2', 'D3', 'B3', 'A3', 'B3', 'D3', 'B3', 'D3', 'G2',...]\n",
    "    with open('data/durations', 'wb') as filepath:\n",
    "        pickle.dump(durations, filepath) \n",
    "else:\n",
    "    with open('data/notes', 'rb') as filepath:\n",
    "        notes = pickle.load(filepath) #['G2', 'D3', 'B3', 'A3', 'B3', 'D3', 'B3', 'D3', 'G2',...]\n",
    "    with open('data/durations', 'rb') as filepath:\n",
    "        durations = pickle.load(filepath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the distinct sets of notes and durations\n",
    "note_names, n_notes = get_distinct(notes)\n",
    "duration_names, n_durations = get_distinct(durations)\n",
    "distincts = [note_names, n_notes, duration_names, n_durations]\n",
    "\n",
    "with open('data/distincts', 'wb') as filepath:\n",
    "    pickle.dump(distincts, filepath)\n",
    "\n",
    "# make the lookup dictionaries for notes and dictionaries and save\n",
    "note_to_int, int_to_note = create_lookups(note_names)\n",
    "duration_to_int, int_to_duration = create_lookups(duration_names)\n",
    "lookups = [note_to_int, int_to_note, duration_to_int, int_to_duration]\n",
    "\n",
    "with open('data/lookups', 'wb') as filepath:\n",
    "    pickle.dump(lookups, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the sequences used by the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_input, network_output = prepare_sequences(notes, durations, lookups, distincts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the structure of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, None, 100)    185200      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, None, 100)    1800        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, None, 200)    0           embedding_7[0][0]                \n",
      "                                                                 embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, None, 256)    467968      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, None, 256)    0           lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 256)          525312      dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 256)          0           lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "notes (Dense)                   (None, 1852)         475964      dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "durations (Dense)               (None, 18)           4626        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,660,870\n",
      "Trainable params: 1,660,870\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_len = None\n",
    "embed_size = 100\n",
    "\n",
    "notes_in = Input(shape = (seq_len,))\n",
    "durations_in = Input(shape = (seq_len,))\n",
    "\n",
    "x1 = Embedding(n_notes, embed_size)(notes_in)\n",
    "x2 = Embedding(n_durations, embed_size)(durations_in) \n",
    "\n",
    "x = Concatenate()([x1,x2])\n",
    "\n",
    "x = LSTM(256, return_sequences=True)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(256)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "notes_out = Dense(n_notes, activation = 'softmax', name = 'notes')(x)\n",
    "durations_out = Dense(n_durations, activation = 'softmax', name = 'durations')(x)\n",
    "\n",
    "model = Model([notes_in, durations_in], [notes_out, durations_out])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "opti = RMSprop(lr = 0.001)\n",
    "model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy']\n",
    "              , loss_weights = [5, 1]\n",
    "              , optimizer=opti\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 237160 samples, validate on 59291 samples\n",
      "Epoch 1/2000000\n",
      "237160/237160 [==============================] - 996s 4ms/step - loss: 16.3442 - notes_loss: 3.1557 - durations_loss: 0.5659 - val_loss: 14.1969 - val_notes_loss: 2.7227 - val_durations_loss: 0.5832\n",
      "Epoch 2/2000000\n",
      "237160/237160 [==============================] - 1040s 4ms/step - loss: 14.1865 - notes_loss: 2.7458 - durations_loss: 0.4573 - val_loss: 14.5362 - val_notes_loss: 2.7850 - val_durations_loss: 0.6110\n",
      "Epoch 3/2000000\n",
      "237160/237160 [==============================] - 1111s 5ms/step - loss: 14.1997 - notes_loss: 2.7548 - durations_loss: 0.4256 - val_loss: 15.0608 - val_notes_loss: 2.8849 - val_durations_loss: 0.6364\n",
      "Epoch 4/2000000\n",
      "237160/237160 [==============================] - 1107s 5ms/step - loss: 14.5289 - notes_loss: 2.8248 - durations_loss: 0.4049 - val_loss: 15.6651 - val_notes_loss: 2.9983 - val_durations_loss: 0.6738\n",
      "Epoch 5/2000000\n",
      "237160/237160 [==============================] - 1120s 5ms/step - loss: 14.4191 - notes_loss: 2.8053 - durations_loss: 0.3926 - val_loss: 16.0227 - val_notes_loss: 3.0689 - val_durations_loss: 0.6785\n",
      "Epoch 6/2000000\n",
      "237160/237160 [==============================] - 1112s 5ms/step - loss: 14.0120 - notes_loss: 2.7260 - durations_loss: 0.3818 - val_loss: 16.3559 - val_notes_loss: 3.1360 - val_durations_loss: 0.6760\n",
      "Epoch 7/2000000\n",
      "237160/237160 [==============================] - 1106s 5ms/step - loss: 13.6882 - notes_loss: 2.6620 - durations_loss: 0.3785 - val_loss: 16.6107 - val_notes_loss: 3.1893 - val_durations_loss: 0.6641\n",
      "Epoch 8/2000000\n",
      "237160/237160 [==============================] - 1104s 5ms/step - loss: 13.3348 - notes_loss: 2.5918 - durations_loss: 0.3759 - val_loss: 16.7167 - val_notes_loss: 3.2101 - val_durations_loss: 0.6661\n",
      "Epoch 9/2000000\n",
      "237160/237160 [==============================] - 1099s 5ms/step - loss: 13.0178 - notes_loss: 2.5284 - durations_loss: 0.3758 - val_loss: 16.9332 - val_notes_loss: 3.2496 - val_durations_loss: 0.6854\n",
      "Epoch 10/2000000\n",
      "237160/237160 [==============================] - 1102s 5ms/step - loss: 12.7981 - notes_loss: 2.4846 - durations_loss: 0.3748 - val_loss: 17.4454 - val_notes_loss: 3.3490 - val_durations_loss: 0.7005\n",
      "Epoch 11/2000000\n",
      "237160/237160 [==============================] - 1105s 5ms/step - loss: 12.5386 - notes_loss: 2.4330 - durations_loss: 0.3737 - val_loss: 17.8988 - val_notes_loss: 3.4418 - val_durations_loss: 0.6901\n",
      "Epoch 12/2000000\n",
      "237160/237160 [==============================] - 1111s 5ms/step - loss: 12.3494 - notes_loss: 2.3956 - durations_loss: 0.3715 - val_loss: 17.9145 - val_notes_loss: 3.4375 - val_durations_loss: 0.7269\n",
      "Epoch 13/2000000\n",
      "237160/237160 [==============================] - 1103s 5ms/step - loss: 12.2220 - notes_loss: 2.3703 - durations_loss: 0.3707 - val_loss: 18.2917 - val_notes_loss: 3.5128 - val_durations_loss: 0.7274\n",
      "Epoch 14/2000000\n",
      "237160/237160 [==============================] - 1101s 5ms/step - loss: 12.0738 - notes_loss: 2.3409 - durations_loss: 0.3693 - val_loss: 18.5047 - val_notes_loss: 3.5611 - val_durations_loss: 0.6994\n",
      "Epoch 15/2000000\n",
      "237160/237160 [==============================] - 1101s 5ms/step - loss: 11.9119 - notes_loss: 2.3083 - durations_loss: 0.3705 - val_loss: 18.6106 - val_notes_loss: 3.5793 - val_durations_loss: 0.7141\n",
      "Epoch 16/2000000\n",
      "237160/237160 [==============================] - 1101s 5ms/step - loss: 11.8020 - notes_loss: 2.2865 - durations_loss: 0.3692 - val_loss: 18.5107 - val_notes_loss: 3.5593 - val_durations_loss: 0.7142\n",
      "Epoch 17/2000000\n",
      "237160/237160 [==============================] - 1101s 5ms/step - loss: 11.7241 - notes_loss: 2.2708 - durations_loss: 0.3699 - val_loss: 18.7660 - val_notes_loss: 3.6112 - val_durations_loss: 0.7100\n",
      "Epoch 18/2000000\n",
      "237160/237160 [==============================] - 1101s 5ms/step - loss: 11.6206 - notes_loss: 2.2504 - durations_loss: 0.3686 - val_loss: 18.9919 - val_notes_loss: 3.6515 - val_durations_loss: 0.7347\n",
      "Epoch 19/2000000\n",
      "237160/237160 [==============================] - 1098s 5ms/step - loss: 11.5201 - notes_loss: 2.2303 - durations_loss: 0.3685 - val_loss: 19.8169 - val_notes_loss: 3.8105 - val_durations_loss: 0.7642\n",
      "Epoch 20/2000000\n",
      "237152/237160 [============================>.] - ETA: 0s - loss: 11.4415 - notes_loss: 2.2143 - durations_loss: 0.3701"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7df2c114830f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m           \u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m           \u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m          )\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint1 = ModelCheckpoint(\n",
    "    \"./weights/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.h5\",\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint2 = ModelCheckpoint(\n",
    "    \"weights.h5\",\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss'\n",
    "    , restore_best_weights=True\n",
    "    , patience = 10\n",
    ")\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    checkpoint1\n",
    "    , checkpoint2\n",
    "    , early_stopping\n",
    " ]\n",
    "\n",
    "model.save_weights('./weights/weights.h5')\n",
    "model.fit(network_input, network_output\n",
    "          , epochs=2000000, batch_size=32\n",
    "          , validation_split = 0.2\n",
    "          , callbacks=callbacks_list\n",
    "          , shuffle=True\n",
    "         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdl",
   "language": "python",
   "name": "gdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
